{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Models&Tokenizers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ],
      "metadata": {
        "id": "77HDATy1OdHg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NGfR4PCjOHZZ"
      },
      "outputs": [],
      "source": [
        "#@ INITIALIZATION: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ],
      "metadata": {
        "id": "oOvUVdpdOlcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INSTALLING DEPENDENCIES: UNCOMMENT BELOW: \n",
        "# !pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "7_xvpbkUOicZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ DOWNLOADING LIBRARIES AND DEPENDENCIES:\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import BertConfig, BertModel\n",
        "from transformers import BertTokenizer\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "SjikyD4ROrCJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRANSFORMER**\n",
        "- I will create the BERT model here. "
      ],
      "metadata": {
        "id": "6ZjshvfyaVm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING BERT MODEL: RANDOM:\n",
        "config = BertConfig()                               # Building BERT Config.\n",
        "model = BertModel(config)                           # Building BERT Model. \n",
        "print(config)                                       # Inspecting Configurations. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpoRHTJmOwk7",
        "outputId": "6355da07-d3cd-4f20-bd49-5bcdec93fc71"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING PRETRAINED MODEL:\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU_WmDpzb1lA",
        "outputId": "2aa2238b-f9ad-48f7-ac5f-3ab13906f8da"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ SAVING THE PRETRAINED BERT MODEL:\n",
        "model.save_pretrained(\"./model\")"
      ],
      "metadata": {
        "id": "3uD2bXb1dyXu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOKENIZERS:**\n",
        "- The basic type of tokenizer that comes to mind is `word-based` tokenizer. It's generally very easy to set up and use with only a few rules, and it yeilds decent results. "
      ],
      "metadata": {
        "id": "5RxMTWEggKeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ EXAMPLE OF WORD BASED TOKENIZATION:\n",
        "tokenized_text = \"I am Thinam Tamang\".split()           # Initializing Tokenization. \n",
        "print(tokenized_text)                                   # Inspecting Tokens. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjXZzYK8d-kv",
        "outputId": "9d7f9053-4d2d-42f4-f7fb-25a3cabe553c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'Thinam', 'Tamang']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUBWORD TOKENIZATION:**\n",
        "- Subword Tokenization rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. "
      ],
      "metadata": {
        "id": "IeeTX9DTxV8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING TOKENIZATION:\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")            # Initializing Tokenizer. \n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")            # Initializing Tokenizer. \n",
        "tokenizer(\"Using a Transformer network is simple !\")                    # Implementing Tokenizer. "
      ],
      "metadata": {
        "id": "dcksc1GOhPI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "933a750f-5faa-4baa-c9d1-053f19682539"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ SAVING THE TOKENIZER:\n",
        "tokenizer.save_pretrained(\"./tokenizer\");"
      ],
      "metadata": {
        "id": "noY9v9P42fyc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENCODING:**\n",
        "- Translating text to numbers is known as `encoding`. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs. "
      ],
      "metadata": {
        "id": "pxCJRDbS4ds8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF TOKENIZATION:\n",
        "sequence = \"Using a Transformer network is simple !\"                    # Initializing Sequence. \n",
        "tokens = tokenizer.tokenize(sequence)                                   # Initializing Tokenization. \n",
        "print(tokens)                                                           # Inspecting Tokens. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83UaJad62v_K",
        "outputId": "a73515d9-4545-42d3-f91d-962416dc0698"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ CONVERSION TO INPUT IDs:\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)                           # Conversion.\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na48g0t676Qr",
        "outputId": "e394a7fb-9de5-48fb-e072-78b8b9e1bf09"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7993, 170, 13809, 23763, 2443, 1110, 3014, 106]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF TOKENIZATION: \n",
        "sequence =[\"I've been waiting for a HuggingFace course my whole life.\",\n",
        "            \"I hate this so much!\"]                                     # Initializing Sequences. \n",
        "tokens = tokenizer.tokenize(sequence)                                   # Initializing Tokenization. \n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)                           # Conversion.\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuC_bCr98-Ck",
        "outputId": "2d13d0fb-51b9-4ad8-bdee-27d84e455352"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[146, 112, 1396, 1151, 2613, 1111, 170, 20164, 10932, 2271, 7954, 1736, 1139, 2006, 1297, 119, 146, 4819, 1142, 1177, 1277, 106]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "biysan099x-Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}