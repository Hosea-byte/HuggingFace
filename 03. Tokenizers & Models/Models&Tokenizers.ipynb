{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Models&Tokenizers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ],
      "metadata": {
        "id": "77HDATy1OdHg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NGfR4PCjOHZZ"
      },
      "outputs": [],
      "source": [
        "#@ INITIALIZATION: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ],
      "metadata": {
        "id": "oOvUVdpdOlcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INSTALLING DEPENDENCIES: UNCOMMENT BELOW: \n",
        "# !pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "7_xvpbkUOicZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ DOWNLOADING LIBRARIES AND DEPENDENCIES:\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import BertConfig, BertModel\n",
        "from transformers import BertTokenizer\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "SjikyD4ROrCJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRANSFORMER**\n",
        "- I will create the BERT model here. "
      ],
      "metadata": {
        "id": "6ZjshvfyaVm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING BERT MODEL: RANDOM: UNCOMMENT BELOW:\n",
        "# config = BertConfig()                               # Building BERT Config.\n",
        "# model = BertModel(config)                           # Building BERT Model. \n",
        "# print(config)                                       # Inspecting Configurations. "
      ],
      "metadata": {
        "id": "BpoRHTJmOwk7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING PRETRAINED MODEL:\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU_WmDpzb1lA",
        "outputId": "bd11fd3d-4455-4237-db4a-b6a9c0ae19b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ SAVING THE PRETRAINED BERT MODEL:\n",
        "model.save_pretrained(\"./model\")"
      ],
      "metadata": {
        "id": "3uD2bXb1dyXu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOKENIZERS:**\n",
        "- The basic type of tokenizer that comes to mind is `word-based` tokenizer. It's generally very easy to set up and use with only a few rules, and it yeilds decent results. "
      ],
      "metadata": {
        "id": "5RxMTWEggKeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ EXAMPLE OF WORD BASED TOKENIZATION:\n",
        "tokenized_text = \"I am Thinam Tamang\".split()           # Initializing Tokenization. \n",
        "print(tokenized_text)                                   # Inspecting Tokens. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjXZzYK8d-kv",
        "outputId": "c10c7e53-409c-4287-dfbd-bfb5a5543ac4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'Thinam', 'Tamang']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUBWORD TOKENIZATION:**\n",
        "- Subword Tokenization rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. "
      ],
      "metadata": {
        "id": "IeeTX9DTxV8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING TOKENIZATION:\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")            # Initializing Tokenizer. \n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")            # Initializing Tokenizer. \n",
        "tokenizer(\"Using a Transformer network is simple !\")                    # Implementing Tokenizer. "
      ],
      "metadata": {
        "id": "dcksc1GOhPI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2e1f2c-6091-44f3-c60f-870de597e030"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ SAVING THE TOKENIZER:\n",
        "tokenizer.save_pretrained(\"./tokenizer\");"
      ],
      "metadata": {
        "id": "noY9v9P42fyc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENCODING:**\n",
        "- Translating text to numbers is known as `encoding`. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs. "
      ],
      "metadata": {
        "id": "pxCJRDbS4ds8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF TOKENIZATION:\n",
        "sequence = \"Using a Transformer network is simple !\"                    # Initializing Sequence. \n",
        "tokens = tokenizer.tokenize(sequence)                                   # Initializing Tokenization. \n",
        "print(tokens)                                                           # Inspecting Tokens. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83UaJad62v_K",
        "outputId": "8a48626a-8b74-47bb-870c-e109dc7629dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ CONVERSION TO INPUT IDs:\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)                           # Conversion.\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na48g0t676Qr",
        "outputId": "655c9b95-8a42-4132-b8e3-42b5514ece16"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7993, 170, 13809, 23763, 2443, 1110, 3014, 106]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF TOKENIZATION: \n",
        "sequence =[\"I've been waiting for a HuggingFace course my whole life.\",\n",
        "            \"I hate this so much!\"]                                     # Initializing Sequences. \n",
        "tokens = tokenizer.tokenize(sequence)                                   # Initializing Tokenization. \n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)                           # Conversion.\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuC_bCr98-Ck",
        "outputId": "59dbbe18-3332-4e7f-f192-a99c0a6f427e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[146, 112, 1396, 1151, 2613, 1111, 170, 20164, 10932, 2271, 7954, 1736, 1139, 2006, 1297, 119, 146, 4819, 1142, 1177, 1277, 106]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batching** is the act of sending multiple sentences through the model, all at once. If we have only one sentence, we can just build a batch with a single sequence. "
      ],
      "metadata": {
        "id": "68a-88dazOCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF TOKENIZATION: UNCOMMENT BELOW: \n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"          # Initialization. \n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)                   # Initializing Tokenizer. \n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)  # Initializing Model.\n",
        "sequence = \"I've been waiting for a Spiderman movie my whole life.\"     # Initialization.\n",
        "tokens = tokenizer.tokenize(sequence)                                   # Encoding. \n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)                           # Converting into Token IDs. \n",
        "input_ids = torch.tensor(ids)                                           # Converting into Tensors. \n",
        "# model(input_ids)                                                      # Inspection. "
      ],
      "metadata": {
        "id": "biysan099x-Z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF TOKENIZATION: ADDING DIMENSION: \n",
        "sequence = \"I've been waiting for a Spiderman movie my whole life.\"     # Initialization.\n",
        "tokens = tokenizer.tokenize(sequence)                                   # Encoding. \n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)                           # Converting into Token IDs. \n",
        "input_ids = torch.tensor([ids])                                         # Converting into Tensors. \n",
        "print(\"Input IDs:\", input_ids)\n",
        "output = model(input_ids)                                               # Implementation of Model. \n",
        "print(\"Logits:\", output.logits)                                         # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FezN5kfqxdo8",
        "outputId": "f3ed1fec-b734-478e-bd5a-5a98fcd20080"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[1045, 1005, 2310, 2042, 3403, 2005, 1037, 6804, 2386, 3185, 2026, 2878,\n",
            "         2166, 1012]])\n",
            "Logits: tensor([[-0.4018,  0.7059]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF TOKENIZATION: ADDING DIMENSION: \n",
        "sequence = \"I've been waiting for a Spiderman movie my whole life.\"     # Initialization.\n",
        "tokens = tokenizer.tokenize(sequence)                                   # Encoding. \n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)                           # Converting into Token IDs. \n",
        "input_ids = torch.tensor([ids, ids])                                    # Converting into Tensors. \n",
        "print(\"Input IDs:\", input_ids)\n",
        "output = model(input_ids)                                               # Implementation of Model. \n",
        "print(\"Logits:\", output.logits)                                         # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFVO9p6FzHRm",
        "outputId": "89c7d400-8872-4c69-de05-ec46fbfef7ce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[1045, 1005, 2310, 2042, 3403, 2005, 1037, 6804, 2386, 3185, 2026, 2878,\n",
            "         2166, 1012],\n",
            "        [1045, 1005, 2310, 2042, 3403, 2005, 1037, 6804, 2386, 3185, 2026, 2878,\n",
            "         2166, 1012]])\n",
            "Logits: tensor([[-0.4018,  0.7059],\n",
            "        [-0.4018,  0.7059]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PADDING THE INPUTS:**\n",
        "- **Padding** makes sure all our sentences have the same length by adding a special word called the `padding token` to the sentences with fewer values. "
      ],
      "metadata": {
        "id": "8kMMCcPw0eTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF PADDING:\n",
        "sequence1_ids = [[200, 200, 200]]                               # Initializing Sequence IDs.  \n",
        "sequence2_ids = [[200, 200]]                                    # Initializing Sequence IDs.  \n",
        "batched_ids = [[200, 200, 200], \n",
        "               [200, 200, tokenizer.pad_token_id]]              # Implementation of Padding. \n",
        "print(model(torch.tensor(sequence1_ids)).logits)                # Inspecting Logits. \n",
        "print(model(torch.tensor(sequence2_ids)).logits)                # Inspecting Logits.\n",
        "print(model(torch.tensor(batched_ids)).logits)                  # Inspecting Logits."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eVFQRy4z_IB",
        "outputId": "511b0684-5479-4f10-db54-06951e458eca"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 1.5694, -1.3895],\n",
            "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ATTENTION MASKS:**\n",
        "- **Attention masks** are tensors with the same shapes as the input IDs tensors, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to i.e. they should be ignored by the attention layers of the model. "
      ],
      "metadata": {
        "id": "WDwbduFU3QDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF PADDING AND ATTENTION MASKS: \n",
        "batched_ids = [[200, 200, 200], \n",
        "               [200, 200, tokenizer.pad_token_id]]              # Implementation of Padding. \n",
        "attention_mask = [[1, 1, 1],\n",
        "                  [1, 1, 0]]                                    # Initialization. \n",
        "outputs = model(torch.tensor(batched_ids),\n",
        "                attention_mask=torch.tensor(attention_mask))    # Implementation of Attention Masks. \n",
        "print(outputs.logits)                                           # Inspection. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnqH24Li2btO",
        "outputId": "2940396e-31bb-4507-d254-11c0476823fa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.5694, -1.3895],\n",
            "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSION:**"
      ],
      "metadata": {
        "id": "MkN8AO80eFLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF TOKENIZER:\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"      # Text Example. \n",
        "model_inputs = tokenizer(sequence)                                          # Initializing Model Inputs. \n",
        "model_inputs = tokenizer(sequence, padding=\"longest\")                       # Padding Upto Maximum Length.\n",
        "model_inputs = tokenizer(sequence, padding=\"max_length\")                    # Padding Upto Model Max Length. \n",
        "model_inputs = tokenizer(sequence, padding=\"max_length\", max_length=8)      # Padding Upto Specified Length. \n",
        "model_inputs = tokenizer(sequence, truncation=True)                         # Truncating Long Sequence. \n",
        "model_inputs = tokenizer(sequence, max_length=8, truncation=True)           # Truncating Long Sequence. \n",
        "model_inputs = tokenizer(sequence, padding=True, return_tensors=\"pt\")       # Return PyTorch Tensors.\n",
        "model_inputs = tokenizer(sequence, padding=True, return_tensors=\"tf\")       # Return TensorFlow Tensors. \n",
        "model_inputs = tokenizer(sequence, padding=True, return_tensors=\"np\")       # Return NumPy Arrays. "
      ],
      "metadata": {
        "id": "gdO7M4Oa4tRn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF TOKENIZER: SPECIAL TOKENS: \n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"      # Text Example. \n",
        "model_inputs = tokenizer(sequence)                                          # Tokenization. \n",
        "tokens = tokenizer.tokenize(sequence)                                       # Initializing Tokens\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)                               # Initializing Input IDs. \n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVEkdP44gzOQ",
        "outputId": "fae56745-4097-4db5-a005-3c27c7d96ec1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ IMPLEMENTATION OF TOKENIZER: DECODING TOKENS: \n",
        "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nqz4P3MhLRT",
        "outputId": "6cf37051-31d1-4c52-81c6-e643af5bf99e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
            "i've been waiting for a huggingface course my whole life.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ TOKENIZER TO MODEL:\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)          # Initializing Model.\n",
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\",\"So\"]  # Text Example. \n",
        "tokens = tokenizer(sequences, padding=True, truncation=True, \n",
        "                   return_tensors=\"pt\")                                         # Initializing Tokenization. \n",
        "output = model(**tokens)                                                        # Implementation of Model. "
      ],
      "metadata": {
        "id": "jSSQfzbRhsLI"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}